





import math
import re
from collections import Counter

import nltk
import numpy as np
import pandas as pd
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from scipy.sparse import csr_matrix
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm


nltk.download("wordnet")
nltk.download("omw-1.4")
nltk.download("averaged_perceptron_tagger")





df = pd.read_csv(
    "ag_news_csv/train.csv", header=None, names=["label", "title", "description"]
)


print(df.head())


def replace_space(word):
    return re.sub(r"[-\\/&]", " ", word)


df["title"] = df["title"].apply(replace_space)
df["description"] = df["description"].apply(replace_space)


def replace_num(word):
    return re.sub(r"\d+", "<NUM>", word)


df["title"] = df["title"].apply(replace_num)
df["description"] = df["description"].apply(replace_num)


def separate_num(word):
    return re.sub(r"(<NUM>)", r" \1 ", word)


df["title"] = df["title"].apply(separate_num)
df["description"] = df["description"].apply(separate_num)


print(df)


def tokenize(text):
    return text.split()


df["tokens"] = df["title"].apply(tokenize) + df["description"].apply(tokenize)


print(df["tokens"])


df.drop("description", axis=1, inplace=True)
df.drop("title", axis=1, inplace=True)


print(df)


def lower(tokens):
    return [word.lower() for word in tokens]


df["tokens"] = df["tokens"].apply(lower)


print(df["tokens"])


def remove_word_suffixes(word):
    if word.endswith("'s"):
        word = word[:-2]
    # elif word.endswith("s"):
    #    word = word[:-1]
    else:
        return re.sub(r'[.,:()\'"?;#$!]', "", word)


def remove_suffixes(tokens):
    return [remove_word_suffixes(word) for word in tokens]


df["tokens"] = df["tokens"].apply(remove_suffixes)


print(df["tokens"])


def remove_stopwords(tokens):
    return [word for word in tokens if (word not in stopwords) and (word is not None)]


with open("stopwords.txt") as file:
    stopwords = file.read().split(",")


print(stopwords)


df["tokens"] = df["tokens"].apply(remove_stopwords)


print(df["tokens"])


def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith("J"):
        return wordnet.ADJ
    elif treebank_tag.startswith("V"):
        return wordnet.VERB
    elif treebank_tag.startswith("N"):
        return wordnet.NOUN
    elif treebank_tag.startswith("R"):
        return wordnet.ADV
    else:
        return wordnet.NOUN


def lemmatize_with_pos(tokens):
    pos_tagged = pos_tag(tokens)
    return [
        lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tagged
    ]


lemmatizer = WordNetLemmatizer()

df["tokens"] = df["tokens"].apply(lemmatize_with_pos)


print(df["tokens"])


df.to_pickle("processed_train_data.pkl")





df = pd.read_pickle("processed_train_data.pkl")


print(df)


words_counter = Counter()
for tokens in df["tokens"]:
    words_counter.update(tokens)
vocabulary = dict(words_counter)


counter = 0
for key, value in vocabulary.items():
    if counter < 20:
        print(f"{key}: {value}")
        counter += 1
    else:
        break


def compute_tf(tokens):
    tf = Counter(tokens)
    for i in tf:
        tf[i] = (1 + math.log10(tf[i])) if tf[i] != 0 else 0
    return dict(tf)


TF = [compute_tf(tokens) for tokens in df["tokens"]]


counter = 0
for i in TF:
    if counter < 10:
        print(i)
        counter += 1
    else:
        break


def compute_idf(dft, df_tokens_len):
    return math.log10(df_tokens_len / dft)


IDF = {word: compute_idf(dft, len(df["tokens"])) for word, dft in vocabulary.items()}


counter = 0
for key, value in IDF.items():
    if counter < 20:
        print(f"{key}: {value}")
        counter += 1
    else:
        break


data = []
indices = []
indptr = [0]


word_list = list(IDF.keys())
word_to_index = {word: i for i, word in enumerate(word_list)}


for i in range(len(TF)):
    for word, tf in TF[i].items():
        if word in IDF:
            tf_idf = tf * IDF[word]
            data.append(tf_idf)
            indices.append((df["label"][i] - 1) * len(IDF) + word_to_index[word])
    indptr.append(len(data))


tf_idf = csr_matrix((data, indices, indptr), shape=(len(TF), len(IDF) * 4), dtype=float)


print(tf_idf)


print(tf_idf.shape)





labels = df["label"]
lambda_vector = np.random.randn(tf_idf.shape[1]) * 0.01
epochs = 50
lr = 0.01
batch_size = 128
num_docs = len(TF)
num_words = len(IDF)


def f_i(word_index, doc_index, label):
    return tf_idf[doc_index][(label - 1) * num_words + word_index]


def p(doc_index, label):
    return np.exp(f_i(document, label).dot(lambda_vector)[0]) / np.exp(
        [f_i(document, label).dot(lambda_vector)[0] for label_prime in range(1, 5)]
    )


def gradient(batch_indices):
    return [
        np.sum([tf_idf[doc_index][lambda_index] for doc_index in range(num_docs)])
        - np.sum(
            [
                np.sum(f_i(tf_idf[doc_index], label_prime))
                * p(tf_idf[doc_index], label_prime)
                for label_prime in range(1, 5)
            ]
        )
        for lambda_index in range(tf_idf.shape[1])
    ]


def log_likelihood():
    return np.sum(
        [f_i(tf_idf[doc_index], labels[doc_index]).dot(lambda_vector)[0]]
        for doc_index in range(num_docs)
    ) - np.sum(
        [
            np.log10(
                np.sum(
                    [
                        f_i(tf_idf[doc_index], label_prime).dot(lambda_vector)[0]
                        for label_prime in range(1, 5)
                    ]
                )
            )
            for doc_index in range(num_docs)
        ]
    )


for t in tqdm(range(epochs), desc="Epochs"):
    shuffled_indices = np.random.permutation(num_docs)

    for start_idx in tqdm(range(0, num_docs, batch_size), desc="Batchs", leave=False):
        end_idx = min(start_idx + batch_size, num_docs)
        batch_indices = shuffled_indices[start_idx:end_idx]

        grad = gradient(batch_indices)
        lambda_vector += lr * np.array(grad)

    ll = log_likelihood()
    print(f"Epoch {t + 1}/{epochs}, Log-Likelihood: {ll:.4f}")


with open("lambda.pkl", "wb") as file:
    pickle.dump(lambda_vector, file)





def predict(lambda_vector, tf_idf):
    scores = tf_idf.dot(lambda_vector)
    probs = softmax(scores)
    return np.argmax(probs, axis=1) + 1


print(tf_idf[0].shape)


print(np.nonzero(tf_idf[1])[0])



